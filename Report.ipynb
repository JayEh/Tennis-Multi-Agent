{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Agent Deep RL Tennis - How it works and what it does\n",
    "\n",
    "Unity ML-Agents Toolkit provides a variety of open-source 3D environments that can be used to train an intelligent agent. In this project we'll look at the multi agent Tennis environment. The objective is to have 2 paddles bounce a ball back and forth, without letting it hit the ground or sending it out of bounds.\n",
    "\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "The learning algorithm being used is based on MADDPG, a multi agent capable version of DDPG, training each agent to compete against the other. The idea behind DDPD/MADDPG is to apply the success of the DQN to continuous actions, something that DQN was not designed to achieve. The researchers behind DDPG were able to solve more than 20 simulated physics tasks with no changes to hyperparameters of their network, demonstrating impressive performance.\n",
    "\n",
    "The MADDPG algorithm uses what are known as actor and critic neural nets to achieve continuous actions, and each agent learns its own actor and critic networks as it plays against the opponent. The actor network predicts the best action and the critic network predicts what the value of that action will be. Considering the success of DQN however and borrowing an idea from there, DDPG also utilizes target networks to stabilize the training which the DDPG team actually described in their paper as being 'crucial'.\n",
    "\n",
    "To cement my understanding I read a lot of online content like blogs, reviewed code written by other people, and read the original research paper.\n",
    "\n",
    "- https://arxiv.org/pdf/1706.02275.pdf\n",
    "- https://www.katnoria.com/maddpg/\n",
    "- https://github.com/katnoria/unityml-tennis/tree/a056fb897200f97d63bef15fda0218ae7941b573\n",
    "- https://github.com/openai/maddpg\n",
    "- https://github.com/kotogasy/unity-ml-tennis\n",
    "\n",
    "This implementation is based on the DDPG implementation from the last project, the code provided in the MADDPG lab exercise, and several online resources.\n",
    "\n",
    "\n",
    "##### Actor network\n",
    "This actor network is based on the same that I used in the DDPG project. I achieved reasonable results with that architecture on the Reacher environment and so carried it over to MADDPG.\n",
    "\n",
    "The actor network is implemented as follows, where state_size is the size of the environment state (33 for Reacher), the output action_size is the number of actions available to the agent (4 for Reacher) and all hidden layers have 128 nodes. \n",
    "\n",
    "This number of hidden nodes was found through trial and error and was found to solve the environment in fewer episodes than other configurations. In the DDPG paper they mention they used 2 hidden layers, as I do here, however they used 400 units on the first and 300 units on the second hidden layers whereas I am using 128 nodes for all hidden layers.\n",
    "    - self.fc1 = nn.Linear(state_size, hidden)\n",
    "    - self.bn1 = nn.BatchNorm1d(hidden)\n",
    "    - self.fc2 = nn.Linear(hidden, hidden)\n",
    "    - self.fc3 = nn.Linear(hidden, action_size)\n",
    "    \n",
    "##### Critic network\n",
    "This critic network is based on the same that I used in the DDPG project. I achieved reasonable results with that architecture on the Reacher environment and so carried it over to MADDPG.\n",
    "\n",
    "The critic network is very similar to the one described in the DDPG paper however during experimenting I made a few changes, and ultimately decided on the following architecture. Just like the network described in the DDPG paper, here the actions are being concatenated as input after the first hidden layer, as opposed to including it with the input to the network.\n",
    "    - self.fc1 = nn.Linear(state_size, hidden)\n",
    "    - self.bn1 = nn.BatchNorm1d(hidden)\n",
    "    - self.fc2 = nn.Linear(hidden+action_size, hidden)\n",
    "    - self.fc3 = nn.Linear(hidden, hidden)\n",
    "    - self.fc4 = nn.Linear(hidden, 1)\n",
    "\n",
    "These networks also use the 'fan-in' type initialization mentioned in the Experiment Details section of the DDPG paper.\n",
    "\n",
    "\n",
    "##### Noise generator\n",
    "While reading the OpenAI Spinning Up page about DDPG they mentioned that basic gaussian noise was often just as good in practice as the OU noise used in the DDPG paper. I decided to experiment a bit and try gaussian noise, which seems to have worked just fine to solve the environment. I also introduced a decay factor so the agent would take fewer exploratory actions as it learned a better policy, the noise method may not be the best option available, but was a fun experiment and still does the trick.\n",
    "\n",
    "The noise function for gaussian noise is straightforward in numpy, where the scale parameter is the standard deviation of the distribution of the noise and size is the size of the output and dictated by the action space (4 for Reacher).\n",
    "    - noise = np.random.normal(loc=0.0, scale=self.scale, size=self.size)\n",
    "   \n",
    "##### Hyperparameter choices\n",
    "To select the hyperparameters to train the network I used a mix of trial and error and best-practice suggestions. \n",
    "\n",
    "The BUFFER_SIZE And BATCH_SIZE parameters were selected through trial and error and were found to enable stable training. The parameter GAMMA it typically set to 0.99 so I continued with that choice, TAU is the soft update parameter determining how much of the local weights to copy to the target network, and uses a slightly higher value than the LR learning rate parameter for training the network via gradient descent. The UPDATE_EVERY and LEARN_TIMES parameters were found through trial and error, it was a challenge to find paratemers that worked well for this environment due to the time needed to test each change.\n",
    "\n",
    "    - BUFFER_SIZE = 200000    # replay buffer size\n",
    "    - BATCH_SIZE = 256        # minibatch size\n",
    "    - GAMMA = 0.99            # discount factor\n",
    "    - TAU = 1e-2              # for soft update of target parameters\n",
    "    - LR = 0.001              # learning rate for the actor and critic networks\n",
    "    - UPDATE_EVERY = 4        # how often to update the network\n",
    "    - LEARN_TIMES = 16        # how many batches to sample and learn from \n",
    "    \n",
    "##### Ideas for Future Improvements\n",
    "There are many things you could do to improve the performance of this algorithm. \n",
    "\n",
    "Prioritized replay could be used to enhance the speed of training and skill of the trained agent, this is a nice low-hanging-fruit type of improvement that would be a sendible next step to improve the agent. The review of my last project suggested a code repo with PER implemented however I did not try to use that for this project. \n",
    "\n",
    "In the MADDPG paper they send additional state to the critic networks to assist with training, however my implementation uses the vanilla DDPG update logic and the critic networks do not share any state information. Another improvement would be to use the same update logic for critic network that is used in the MADDPG paper, as opposed to this simpler approach that simply uses 2 DDPG agents. \n",
    "\n",
    "Or we could try out a different algorithm entirely, multi-agent PPO would be another good choice for this environment.\n",
    "\n",
    "The hyper parameters could likely use some additional tweaking too, but it's very time consuming to test those changes so I've settled with the first set of parameters that I found working.\n",
    "\n",
    "\n",
    "### Let's train an agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from maddpg_agent import MADDPG\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "def new_unity_environment(train_mode=True):\n",
    "    env = UnityEnvironment(file_name=\".//Tennis_Windows_x86_64//Tennis.exe\")\n",
    "    \n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "    \n",
    "    # get the current state\n",
    "    state = env_info.vector_observations[0]\n",
    "    \n",
    "    # look up the size of the action and state spaces\n",
    "    state_size = env_info.vector_observations[0].shape[0]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    \n",
    "    return (brain_name, env, env_info, state, state_size, action_size)\n",
    "\n",
    "\n",
    "def maddpg_train(maddpg, env, brain_name, state_size, train_mode=True, n_episodes=10000, max_t=1000):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # reset the environment for the start of a new episode\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "        \n",
    "        # get the current state for all agents\n",
    "        state = env_info.vector_observations\n",
    "        \n",
    "        # the score each episode starts at zero\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = maddpg.act(state, add_noise=True)\n",
    "            env_info = env.step(actions)[brain_name] # containing actions for both agents\n",
    "            next_state = env_info.vector_observations # next states for both agents\n",
    "            rewards = env_info.rewards  # rewards for both agents\n",
    "            dones = env_info.local_done # done status for both agents\n",
    "            score += max(rewards)       # take only the highest score \n",
    "            maddpg.step(state, actions, rewards, next_state, dones)\n",
    "            state = next_state\n",
    "            if any(dones):  # if either agent is done, you're done!\n",
    "                break \n",
    "            \n",
    "        scores_window.append(score)       # save most recent max score\n",
    "        scores.append(score)              \n",
    "        mean_score = np.mean(scores_window)\n",
    "                \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tActual Score:{:.2f}'.format(i_episode, mean_score, score), end=\"\")\n",
    "        if i_episode % 10 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\tActual Score:{:.2f}'.format(i_episode, mean_score, score))\n",
    "        if np.mean(scores_window)>=0.50:  # solved is 0.5\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\\tActual Score:{:.2f}'.format(i_episode, mean_score, score))\n",
    "            for i, agent in enumerate(maddpg.agents):\n",
    "                torch.save(agent.actor_local.state_dict(), 'Z:/{:.2f}_actor_{}_checkpoint.pth'.format(mean_score, i))\n",
    "            break  # or not and just keep on keepin on\n",
    "            \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 0.01\tActual Score:0.00\n",
      "Episode 20\tAverage Score: 0.03\tActual Score:0.10\n",
      "Episode 30\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 40\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 50\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 60\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 70\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 80\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 90\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 100\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 110\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 120\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 130\tAverage Score: 0.02\tActual Score:0.00\n",
      "Episode 140\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 150\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 160\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 170\tAverage Score: 0.03\tActual Score:0.10\n",
      "Episode 180\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 190\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 200\tAverage Score: 0.03\tActual Score:0.00\n",
      "Episode 210\tAverage Score: 0.04\tActual Score:0.10\n",
      "Episode 220\tAverage Score: 0.04\tActual Score:0.00\n",
      "Episode 230\tAverage Score: 0.04\tActual Score:0.00\n",
      "Episode 240\tAverage Score: 0.04\tActual Score:0.20\n",
      "Episode 250\tAverage Score: 0.05\tActual Score:0.10\n",
      "Episode 260\tAverage Score: 0.05\tActual Score:0.10\n",
      "Episode 270\tAverage Score: 0.05\tActual Score:0.00\n",
      "Episode 280\tAverage Score: 0.05\tActual Score:0.00\n",
      "Episode 290\tAverage Score: 0.05\tActual Score:0.10\n",
      "Episode 300\tAverage Score: 0.05\tActual Score:0.00\n",
      "Episode 310\tAverage Score: 0.05\tActual Score:0.00\n",
      "Episode 320\tAverage Score: 0.06\tActual Score:0.00\n",
      "Episode 330\tAverage Score: 0.06\tActual Score:0.00\n",
      "Episode 340\tAverage Score: 0.06\tActual Score:0.30\n",
      "Episode 350\tAverage Score: 0.06\tActual Score:0.10\n",
      "Episode 360\tAverage Score: 0.06\tActual Score:0.00\n",
      "Episode 370\tAverage Score: 0.06\tActual Score:0.10\n",
      "Episode 380\tAverage Score: 0.06\tActual Score:0.10\n",
      "Episode 390\tAverage Score: 0.06\tActual Score:0.10\n",
      "Episode 400\tAverage Score: 0.07\tActual Score:0.40\n",
      "Episode 410\tAverage Score: 0.07\tActual Score:0.00\n",
      "Episode 420\tAverage Score: 0.08\tActual Score:0.10\n",
      "Episode 430\tAverage Score: 0.08\tActual Score:0.00\n",
      "Episode 440\tAverage Score: 0.07\tActual Score:0.10\n",
      "Episode 450\tAverage Score: 0.07\tActual Score:0.10\n",
      "Episode 460\tAverage Score: 0.09\tActual Score:0.00\n",
      "Episode 470\tAverage Score: 0.09\tActual Score:0.00\n",
      "Episode 480\tAverage Score: 0.09\tActual Score:0.30\n",
      "Episode 490\tAverage Score: 0.10\tActual Score:0.10\n",
      "Episode 500\tAverage Score: 0.10\tActual Score:0.50\n",
      "Episode 510\tAverage Score: 0.10\tActual Score:0.00\n",
      "Episode 520\tAverage Score: 0.10\tActual Score:0.10\n",
      "Episode 530\tAverage Score: 0.10\tActual Score:0.00\n",
      "Episode 540\tAverage Score: 0.10\tActual Score:0.10\n",
      "Episode 550\tAverage Score: 0.09\tActual Score:0.00\n",
      "Episode 560\tAverage Score: 0.09\tActual Score:0.10\n",
      "Episode 570\tAverage Score: 0.09\tActual Score:0.10\n",
      "Episode 580\tAverage Score: 0.09\tActual Score:0.10\n",
      "Episode 590\tAverage Score: 0.09\tActual Score:0.00\n",
      "Episode 600\tAverage Score: 0.09\tActual Score:0.20\n",
      "Episode 610\tAverage Score: 0.09\tActual Score:0.10\n",
      "Episode 620\tAverage Score: 0.09\tActual Score:0.10\n",
      "Episode 630\tAverage Score: 0.11\tActual Score:0.00\n",
      "Episode 640\tAverage Score: 0.12\tActual Score:0.10\n",
      "Episode 650\tAverage Score: 0.13\tActual Score:0.10\n",
      "Episode 660\tAverage Score: 0.13\tActual Score:0.00\n",
      "Episode 670\tAverage Score: 0.13\tActual Score:0.00\n",
      "Episode 680\tAverage Score: 0.13\tActual Score:0.39\n",
      "Episode 690\tAverage Score: 0.14\tActual Score:0.10\n",
      "Episode 700\tAverage Score: 0.13\tActual Score:0.00\n",
      "Episode 710\tAverage Score: 0.14\tActual Score:0.10\n",
      "Episode 720\tAverage Score: 0.14\tActual Score:0.00\n",
      "Episode 730\tAverage Score: 0.15\tActual Score:0.10\n",
      "Episode 740\tAverage Score: 0.16\tActual Score:0.50\n",
      "Episode 750\tAverage Score: 0.16\tActual Score:0.10\n",
      "Episode 760\tAverage Score: 0.18\tActual Score:0.30\n",
      "Episode 770\tAverage Score: 0.18\tActual Score:0.30\n",
      "Episode 780\tAverage Score: 0.21\tActual Score:0.30\n",
      "Episode 790\tAverage Score: 0.23\tActual Score:0.50\n",
      "Episode 800\tAverage Score: 0.24\tActual Score:0.10\n",
      "Episode 810\tAverage Score: 0.24\tActual Score:0.39\n",
      "Episode 820\tAverage Score: 0.24\tActual Score:0.20\n",
      "Episode 830\tAverage Score: 0.24\tActual Score:0.10\n",
      "Episode 840\tAverage Score: 0.23\tActual Score:0.10\n",
      "Episode 850\tAverage Score: 0.22\tActual Score:0.30\n",
      "Episode 860\tAverage Score: 0.22\tActual Score:0.10\n",
      "Episode 870\tAverage Score: 0.24\tActual Score:0.00\n",
      "Episode 880\tAverage Score: 0.26\tActual Score:0.00\n",
      "Episode 890\tAverage Score: 0.30\tActual Score:0.10\n",
      "Episode 900\tAverage Score: 0.32\tActual Score:0.20\n",
      "Episode 910\tAverage Score: 0.37\tActual Score:5.20\n",
      "Episode 920\tAverage Score: 0.39\tActual Score:0.50\n",
      "Episode 930\tAverage Score: 0.50\tActual Score:1.50\n",
      "\n",
      "Environment solved in 930 episodes!\tAverage Score: 0.50\tActual Score:1.50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "brain_name, env, env_info, state, state_size, action_size = new_unity_environment(train_mode=True)\n",
    "maddpg = MADDPG(state_size, action_size, 1337)\n",
    "scores = maddpg_train(maddpg, env, brain_name, state_size, train_mode=True)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores after training to a 100 episode average score of 0.5\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watch a trained agent!\n",
    "\n",
    "Run the code cell below to watch a trained agent in the Unity environment. We'll load the saved weights for the actor networks, copy those weights into the corresponding target networks, then start up the agent and watch it perform.\n",
    "\n",
    "Run the next code cell to see the trained agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7000000104308128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from maddpg_agent import  MADDPG\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "\n",
    "# create the new unity environment\n",
    "brain_name, env, env_info, state, state_size, action_size = new_unity_environment(train_mode=False)\n",
    "\n",
    "# create an instance of the 2 agent algorithm\n",
    "maddpg_agent = MADDPG(state_size=state_size, action_size=action_size)\n",
    "\n",
    "# for each agent, load its saved weights\n",
    "maddpg_agent.agents[0].actor_local.load_state_dict(torch.load('./0.50_actor_0_checkpoint.pth'))\n",
    "maddpg_agent.agents[1].actor_local.load_state_dict(torch.load('./0.50_actor_1_checkpoint.pth'))\n",
    "\n",
    "maddpg_agent.agents[0].copy_weights_from_local_to_target()\n",
    "maddpg_agent.agents[1].copy_weights_from_local_to_target()\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations               # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "\n",
    "# watch a trained agent!\n",
    "while True:\n",
    "    actions = maddpg_agent.act(state, add_noise=False)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_state = env_info.vector_observations\n",
    "    rewards = env_info.rewards  \n",
    "    dones = env_info.local_done \n",
    "    score += max(rewards)       # take only the highest score\n",
    "    maddpg_agent.step(state, actions, rewards, next_state, dones)\n",
    "    state = next_state\n",
    "    \n",
    "    if any(dones):  # if either agent is done, you're done!\n",
    "        env.close()\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
